{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dotenv is a library that allows us to securely load env variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# used to load an individual file (TextLoader) or multiple files (DirectoryLoader)\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "\n",
    "# used to split the text within documents and chunk the data\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# use embedding from OpenAI (but others available)\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# using Chroma database to store our vector embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# use this to configure the Chroma database\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# we'll use the chain that allows Question and Answering and provides source of where it got the data from. This is useful if you have multiple files. If you don't need the source, you can use RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "# we'll use the OpenAI Chat model to interact with the embeddings. This is the model that allows us to query in a similar way to ChatGPT\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# we'll need this for reading/storing from directories\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds .env file and loads the vars\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = \"doc_gpt.ipynb\"\n",
    "# get absolute path\n",
    "FULL_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "CURRENT_PATH = FULL_PATH.split(\"doc_gpt\")[0]\n",
    "\n",
    "# get the path of the db with docs\n",
    "DB_DIR = os.path.join(CURRENT_PATH, \"db\")\n",
    "\n",
    "# this can change to a path with files such as publications, meeting minutes etc.\n",
    "DATA_DIR = os.path.join(CURRENT_PATH, \"data\")\n",
    "\n",
    "DB_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load individual files\n",
    "doc_loader = TextLoader(f\"{DATA_DIR}/MSFT_Call_Transcript.txt\", encoding=\"utf8\")\n",
    "\n",
    "# use directory loader for dirs\n",
    "# doc_loader = DirectoryLoader(DB_DIR)\n",
    "\n",
    "# load the document\n",
    "document = doc_loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a splitter with relevant parameters\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "\n",
    "# split the doc data\n",
    "split_docs = text_splitter.split_documents(document)\n",
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from OpenAI\n",
    "openai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure DB\n",
    "client_settings = Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",  # store parquet files/DuckDB\n",
    "    persist_directory=DB_DIR,\n",
    "    anonymized_telemetry=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class level var vector store\n",
    "vector_store = None\n",
    "\n",
    "# check if db exists already\n",
    "# if not, create ig\n",
    "if not os.path.exists(DB_DIR):\n",
    "    vector_store = Chroma.from_documents(\n",
    "        split_docs,\n",
    "        openai_embeddings,\n",
    "        persist_directory=DB_DIR,\n",
    "        client_settings=client_settings,\n",
    "        collection_name=\"transcript_store\",\n",
    "    )\n",
    "    vector_store.persist()\n",
    "else:\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"transcript_store\",\n",
    "        persist_directory=DB_DIR,\n",
    "        embedding_function=openai_embeddings,\n",
    "        client_settings=client_settings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and configure our chain\n",
    "# we're using ChatOpenAI LLM with the 'gpt-3.5-turbo' model\n",
    "# we're setting the temperature to 0. The higher the temperature, the more 'creative' the answers. In my case, I want as factual and direct from source info as possible\n",
    "# 'stuff' is the default chain_type which means it uses all the data from the document\n",
    "# set the retriever to be our embeddings database\n",
    "qa_with_source = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_document(question):\n",
    "    return qa_with_source({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"What is your query? \", end=\"\")\n",
    "    user_query = input(\"\\033[33m\")\n",
    "    print(\"\\033[0m\")\n",
    "    if user_query == \"quit\":\n",
    "        break\n",
    "    response = query_document(user_query)\n",
    "    # make the answer green and source blue using ANSI codes\n",
    "    print(f'Answer: \\033[32m{response[\"answer\"]}\\033[0m')\n",
    "    print(f'\\033[34mSources: {response[\"sources\"]}\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
